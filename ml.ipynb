{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![head.png](res/head.jpg)\n",
    "\n",
    "# Machine learning\n",
    "\n",
    "**Prof. Dr. Fabian Woebbeking**\n",
    "</br>\n",
    "Assistant Professor of Financial Economics\n",
    "\n",
    "IWH - Leibniz Institute for Economic Research,\n",
    "</br>\n",
    "MLU - Martin Luther University Halle-Wittenberg\n",
    "\n",
    "[fabian.woebbeking@iwh-halle.de](mailto:fabian.woebbeking@iwh-halle.de)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Relevant imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "Books:\n",
    "\n",
    "* Müller and Guido. Introduction to machine learning with Python: a guide for data scientists.\n",
    "* Hastie, Tibshirani and Friedman. The elements of statistical learning: data mining, inference, and prediction.\n",
    "* Kuhn and Johnson. Applied predictive modeling.\n",
    "* Hal Daumé III. A Course in Machine Learning (Free full text: http://ciml.info/)\n",
    "\n",
    "Paper:\n",
    "* Athey, S. \"Beyond prediction: Using big data for policy problems.\" Science 355.6324 (2017): 483-485.\n",
    "* Athey, S. The Impact of Machine Learning on Economics. The economics of artificial intelligence. University of Chicago Press, 2019. 507-552.\n",
    "* Athey, S., and G. W. Imbens. Machine learning methods that economists should know about. Annual Review of Economics 11 (2019): 685-725.\n",
    "* Mullainathan, S., and J. Spiess. Machine learning: an applied econometric approach. Journal of Economic Perspectives 31.2 (2017): 87-106."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ML in economic research\n",
    "\n",
    "* Bajari, P., Nekipelov, D., Ryan, S. P., and M. Yang. Machine learning methods for demand estimation. American Economic Review 105.5 (2015): 481-85.\n",
    "* Burgess, R., Hansen, M., Olken, B. A., and P. Potapov. The Political Economy of Deforestation in the Tropics. Quarterly Journal of Economics 127.4 (2012): 1707-54.\n",
    "* Engl, F., Riedl, A., and R. Weber. Spillover Effects of Institutions on Cooperative Behavior, Preferences, and Beliefs. Amercian Economic Journal: Microeconomics 13.4 (2021): 261-99.\n",
    "* Barth, A., Sasan M., and F. Woebbeking. \"Let Me Get Back to You” - A Machine Learning Approach to Measuring NonAnswers. Management Science 69.10 (2023): 6333-6348."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "* https://scikit-learn.org/stable/user_guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised learning\n",
    "\n",
    "Some models: https://scikit-learn.org/stable/supervised_learning.html\n",
    "\n",
    "![PhD_Session_ML.jpg](res/PhD_Session_ML.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Unsupervised\n",
    "\n",
    "Some models: https://scikit-learn.org/stable/unsupervised_learning.html\n",
    "\n",
    "![PhD_Session_M_unsupL.jpg](res/PhD_Session_M_unsupL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reinforcement learning\n",
    "\n",
    "![alphago.jpg](res/ago.png)\n",
    "\n",
    "(Watch AlphaGo on Netflix, Prime, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Selling your black box\n",
    "\n",
    "* https://scikit-learn.org/stable/model_selection.html\n",
    "\n",
    "* Athey, S. \"Beyond prediction: Using big data for policy problems.\" Science 355.6324 (2017): 483-485."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cross Validation\n",
    "\n",
    "![grid_search_cross_validation.png](res/grid_search_cross_validation.png)\n",
    "\n",
    "(Source: https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explainable AI (XAI)\n",
    "\n",
    "Various techniques aiming at\n",
    "\n",
    "* Model interpretability (vs black box)\n",
    "* Feature importance:\n",
    "  * SHAP (SHapley Additive exPlanations)\n",
    "  * LIME (Local Interpretable Model-agnostic Explanations) \n",
    "* Visualization\n",
    "\n",
    "Zoo of methods, e.g. LIME, Anchors, GraphLIME, LRP, DTD, PDA, TCAV, XGNN, SHAP, ASV, Break-Down, Shapley Flow, Textual Explanations of Visual Models, Integrated Gradients, Causal Models, Meaningful Perturbations, and X-NeSyL ... see:\n",
    "\n",
    "* Holzinger, Andreas, et al. \"Explainable AI methods-a brief overview.\" International Workshop on Extending Explainable AI Beyond Deep Models and Classifiers. Cham: Springer International Publishing, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Git(ing) ready\n",
    "\n",
    "* http://rogerdudler.github.io/git-guide/\n",
    "\n",
    "* https://guides.github.com/\n",
    "\n",
    "* https://git-scm.com/book/en/v2\n",
    "\n",
    "* https://peps.python.org/pep-0008/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![head.png](res/final_doc.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Git and GitHub\n",
    "\n",
    "![git.png](res/git.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Git (local repository)\n",
    "> Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. [(see Git, 2023)](https://git-scm.com/)\n",
    "\n",
    "Some source-code editors come with build-in Git (and even GitHub) capabilities or can be extended (e.g. [Microsoft's Visual Studio Code](https://code.visualstudio.com/), which I use during this course).\n",
    "\n",
    "Your local repository is essentially a folder on your local file system. Changes made in that folder can be committed to the (local) git repository. \n",
    "\n",
    "First, \"stage\" your changes - this is sth. like a pre-commit:\n",
    "\n",
    "```Bash\n",
    "# The '*' adds all changes made in your local folder\n",
    "git add *\n",
    "```\n",
    "\n",
    "Second, commit your staged changes to the local repository:\n",
    "\n",
    "```Bash\n",
    "git commit -m 'Commit message'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the broadest sense, you could see Git as a block chain of commits (changes) made to your repository. You can thus\n",
    "* observe a complete (almost immutable) history.\n",
    "* `git checkout` the state at any commit to the repository.\n",
    "\n",
    "More on Git:\n",
    "\n",
    "* About Git itself: https://git-scm.com/about\n",
    "* Getting started (videos, tutorials): https://git-scm.com/doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GitHub (remote repository)\n",
    "\n",
    "> GitHub is an Internet hosting service for software development and version control using Git.\n",
    "\n",
    "Connecting to the remote repository\n",
    "\n",
    "After installing git, you can `clone` the course repository to your local system:\n",
    "\n",
    "```Bash\n",
    "git clone https://github.com/cafawo/FinancialDataAnalytics.git\n",
    "```\n",
    "\n",
    "Your local Git repository remembers its origins. This enables you to `pull` updates from the remote (Git does not synchronize automatically). \n",
    "\n",
    "```Bash\n",
    "git pull\n",
    "```\n",
    "\n",
    "If you have write access to the remote, you can also `push` changes to it.\n",
    "\n",
    "```Bash\n",
    "git push\n",
    "```\n",
    "\n",
    "Careful: Git tries its best to merge the remote with the local repository, however, might fail if the two repositories are 'too' diverging. This should not concern you too much as a single user, but becomes very relevant when collaborating on a remote.\n",
    "\n",
    "This is all we need for this course, however, it is only the tip of the iceberg. More on GitHub:\n",
    "\n",
    "* Working with GitHub (remotes): https://skills.github.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Coding style\n",
    "\n",
    "Without being too pedantic, we follow the [PEP 8 – Style Guide for Python Code](https://peps.python.org/pep-0008/). When in doubt, return to this source for guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Naming convention\n",
    "\n",
    "Here are some best practices to follow when naming stuff.\n",
    "* Use all lowercase. Ex: `name` instead of `Name`\n",
    "* One exception: class names should start with a capital letter and follow by lowercase letters.\n",
    "* Use snake_case convention (i.e., separate words by underscores, look like a snake). Ex: ``gross_profit`` instead of ``grossProfit`` or ``GrossProfit``.\n",
    "* Should be meaningful and easy to remember. Ex: ``interest_rate`` instead of ``r`` or ``ir``.\n",
    "* Should have a reasonable length. Ex: ``sales_apr`` instead of ``sales_data_for_april``\n",
    "* Avoid names of popular functions and modules. Ex: avoid ``print``, ``math``, or ``collections``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comments\n",
    "\n",
    "Comments should help to understand how your code works and your intentions behind it! \n",
    "\n",
    "> Comments that contradict the code are worse than no comments. Always make a priority of keeping the comments up-to-date when the code changes! Comments should be complete sentences. The first word should be capitalized, unless it is an identifier that begins with a lower case letter (never alter the case of identifiers!). [(PEP 8)](https://peps.python.org/pep-0008/#comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural language processing (NLP)\n",
    "\n",
    "How to squeeze textual data into machine learning methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bag of words\n",
    "\n",
    "Taddy, Matt. **\"Multinomial inverse regression for text analysis.\"** Journal of the American Statistical Association 108.503 (2013): 755-770."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ants</th>\n",
       "      <th>because</th>\n",
       "      <th>do</th>\n",
       "      <th>get</th>\n",
       "      <th>how</th>\n",
       "      <th>that</th>\n",
       "      <th>want</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Do you want ants?</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Because that’s how you get ants.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ants  because  do  get  how  that  want  you\n",
       "Do you want ants?                    1        0   1    0    0     0     1    1\n",
       "Because that’s how you get ants.     1        1   0    1    1     1     0    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text input\n",
    "malory = [\"Do you want ants?\",\n",
    "          \"Because that’s how you get ants.\"]\n",
    "\n",
    "# All unique tokens from the text input (here words, could be n-grams)\n",
    "feature_names = ['ants', 'because', 'do', 'get', 'how', 'that', 'want', 'you']\n",
    "\n",
    "feature_matrix = np.array([[1, 0, 1, 0, 0, 0, 1, 1],\n",
    "                           [1, 1, 0, 1, 1, 1, 0, 1]])\n",
    "\n",
    "display(pd.DataFrame(feature_matrix, columns=feature_names, index=malory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Word embedding\n",
    "\n",
    "Mikolov, Tomas, et al. **\"Distributed representations of words and phrases and their compositionality.\"** Advances in neural information processing systems 26 (2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example: t-distributed Stochastic Neighbor Embedding (TSNE)\n",
    "\n",
    "![logo.png](res/tsne.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris|France: 0.99\n",
      "Paris|Berlin: 0.93\n",
      "Paris|Italy:  0.83\n"
     ]
    }
   ],
   "source": [
    "# Simplified word embeddings for cities and countries\n",
    "word_embeddings = {\n",
    "    \"Paris\": np.array([0.8, 0.2, 0.1]),\n",
    "    \"France\": np.array([0.7, 0.2, 0.2]),\n",
    "    \"Berlin\": np.array([0.6, 0.4, 0.2]),\n",
    "    \"Germany\": np.array([0.6, 0.3, 0.3]),\n",
    "    \"Rome\": np.array([0.5, 0.6, 0.2]),\n",
    "    \"Italy\": np.array([0.5, 0.5, 0.3])\n",
    "}\n",
    "\n",
    "# Function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    dot_product = np.dot(vec_a, vec_b)\n",
    "    norm_a = np.linalg.norm(vec_a)\n",
    "    norm_b = np.linalg.norm(vec_b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "print(f\"Paris|France: {cosine_similarity(word_embeddings['Paris'], word_embeddings['France']):.2f}\")\n",
    "print(f\"Paris|Berlin: {cosine_similarity(word_embeddings['Paris'], word_embeddings['Berlin']):.2f}\")\n",
    "print(f\"Paris|Italy:  {cosine_similarity(word_embeddings['Paris'], word_embeddings['Italy']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Berlin' - 'Germany' + 'France' = 'Paris' (0.90)\n"
     ]
    }
   ],
   "source": [
    "# Vector arithmetic: Berlin - Germany + France\n",
    "result_vector = word_embeddings[\"Berlin\"] - word_embeddings[\"Germany\"] + word_embeddings[\"France\"]\n",
    "\n",
    "# Find the closest word to the resulting vector\n",
    "closest_city = None\n",
    "max_similarity = -1\n",
    "for word in [\"Paris\", \"Rome\", \"Italy\"]: \n",
    "    similarity = cosine_similarity(result_vector, word_embeddings[word])\n",
    "    if similarity > max_similarity:\n",
    "        max_similarity = similarity\n",
    "        closest_word = word\n",
    "\n",
    "print(f\"'Berlin' - 'Germany' + 'France' = '{closest_word}' ({similarity:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformer architecture\n",
    "\n",
    "Vaswani, Ashish, et al. **\"Attention is all you need.\"** Advances in neural information processing systems 30 (2017).\n",
    "\n",
    "Check out: https://huggingface.co/\n",
    "\n",
    "![logo.png](res/moore.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "word_embeddings = {\n",
    "    \"Paris\": np.array([0.8, 0.2, 0.1]),\n",
    "    \"France\": np.array([0.7, 0.3, 0.2]),\n",
    "    \"Berlin\": np.array([0.6, 0.1, 0.2]),\n",
    "    \"Germany\": np.array([0.6, 0.4, 0.3]),\n",
    "    \"Rome\": np.array([0.5, 0.2, 0.2]),\n",
    "    \"Italy\": np.array([0.5, 0.5, 0.3])\n",
    "    }\n",
    "\n",
    "# This code is just illustrative ... look at the steps, not the code!\n",
    "def transformer_encoder(word_embeddings):\n",
    "    # Step 1: Input Embedding\n",
    "    word_embeddings = word_embeddings\n",
    "\n",
    "    # Step 2: Positional Encoding\n",
    "    positional_embeddings = {word: vec + 0.1 for word, vec in word_embeddings.items()}\n",
    "\n",
    "    # Step 3: Attention\n",
    "    attention_sum = sum(positional_embeddings.values())\n",
    "    attention_output = {word: vec * attention_sum for word, vec in positional_embeddings.items()}\n",
    "\n",
    "    # Step 4: Feed-Forward Network\n",
    "    feed_forward_output = {word: vec + np.array([0.3, 0.3, 0.3]) for word, vec in attention_output.items()}\n",
    "\n",
    "    return feed_forward_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Positional Encoding: This step modifies the original word embeddings by adding a positional value to each element. In real transformers, positional encoding is crucial because it provides information about the position of each word in the sequence. Our simple version just adds 0.1 to every element, but real models use more complex functions for positional encoding.\n",
    "\n",
    "Attention: The self-attention mechanism in transformers allows each position in the encoder to consider every other position in the input sequence when computing its representation. In our example, we simulate self-attention by summing all the positional embeddings and then multiplying each embedding with this sum. This is a vast simplification and doesn't truly represent the selective attention mechanism used in real transformers.\n",
    "\n",
    "Feed-Forward Network: In an actual transformer, each position's output from the self-attention layer is processed by a feed-forward neural network. This network consists of linear transformations and non-linear activations, allowing the model to learn complex transformations of the data. In our example, we simplify this by adding a fixed array [0.3, 0.3, 0.3] to each embedding vector, simulating a very basic transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is a City\n",
      "France is a Country\n",
      "Berlin is a City\n",
      "Germany is a Country\n",
      "Rome is a City\n",
      "Italy is a Country\n"
     ]
    }
   ],
   "source": [
    "def classify_city_country(transformed_embeddings):\n",
    "    classification = {}\n",
    "    for word, vec in transformed_embeddings.items():\n",
    "        # Classification rule: if the second element is greater than 1.2, classify as Country, else as City\n",
    "        classification[word] = \"Country\" if vec[1] > 1.2 else \"City\"\n",
    "    return classification\n",
    "\n",
    "\n",
    "# Process the embeddings through the transformer encoder\n",
    "transformed_embeddings = transformer_encoder(word_embeddings)\n",
    "\n",
    "# Classify each word as City or Country\n",
    "classification = classify_city_country(transformed_embeddings)\n",
    "\n",
    "# Displaying the classification results\n",
    "for word, category in classification.items():\n",
    "    print(f\"{word} is a {category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ChatGPT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI currently offers 32 models, e.g.:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Model(id='curie-search-query', created=1651172509, object='model', owned_by='openai-dev'),\n",
       " Model(id='gpt-3.5-turbo-0301', created=1677649963, object='model', owned_by='openai'),\n",
       " Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'),\n",
       " Model(id='babbage-002', created=1692634615, object='model', owned_by='system'),\n",
       " Model(id='ada-search-query', created=1651172505, object='model', owned_by='openai-dev'),\n",
       " Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'),\n",
       " Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use your password to log in here ...\n",
    "with open('gptpassword.txt', 'r') as file:\n",
    "    openai.api_key = file.read().strip()\n",
    "\n",
    "#returns a list of all OpenAI models\n",
    "models = openai.models.list()\n",
    "print(f\"OpenAI currently offers {len(models.data)} models, e.g.:\")\n",
    "display(models.data[0::5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \n",
    "    \"You are a helpful assistant.\"}]\n",
    "messages.append({\"role\": \"user\", \"content\": \n",
    "    \"Classify into two categories, namely, 'City' and 'Country'\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \n",
    "    \"Classify this: Germany, Paris, France, Berlin, Rome, Italy\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \n",
    "    \"The output should be in JSON format.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"City\": [\"Paris\", \"Berlin\", \"Rome\"],\n",
      "  \"Country\": [\"Germany\", \"France\", \"Italy\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Send prompt to API and retrieve results\n",
    "completion = openai.chat.completions.create(\n",
    "                model=\"gpt-4\", temperature=0.0, seed=2024, messages=messages\n",
    "            )\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API parameters https://platform.openai.com/docs/api-reference\n",
    "* temperature: \"What sampling temperature to use, between 0 and 2. Lower values will make it more focused and deterministic.\"\n",
    "* seed: \"This feature is in Beta. If specified, **our system will make a best effort to sample deterministically**, such that repeated requests with the same seed and parameters should return the same result.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Many thanks and happy coding!\n",
    "\n",
    "[fabian.woebbeking@iwh-halle.de](mailto:fabian.woebbeking@iwh-halle.de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jupyter nbconvert vdt.ipynb --to slides --post serve --SlidesExporter.reveal_scroll=True\n",
    "local_dir = %pwd\n",
    "os.system(f'jupyter nbconvert {local_dir}/ml.ipynb --to slides --SlidesExporter.reveal_scroll=True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Generate PDF\n",
    "#os.system(f'jupyter nbconvert {local_dir}/ml.ipynb --to pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "title": "Machine Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
